#!/usr/bin/env python3
"""
åŸºäºLLMçš„è‡ªåŠ¨è¯é¢˜å»ºæ¨¡+è¯„ä¼°+é—­ç¯ä¼˜åŒ–ç³»ç»Ÿ
ä¸»æ§åˆ¶è„šæœ¬
"""

import os
import sys
import json
import argparse
from datetime import datetime
from typing import Dict, Any

# æ·»åŠ è‡ªå®šä¹‰æ¨¡å—è·¯å¾„
sys.path.append('./custom_pipeline')

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡å’Œä¾èµ–"""
    print("ğŸ”§ è®¾ç½®ç¯å¢ƒ...")
    
    # æ£€æŸ¥å¿…è¦çš„ç›®å½•
    directories = [
        'data/input',
        'data/output',
        'results',
        'prompts',
        'logs'
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env_vars = [
        'OPENAI_API_KEY',
        'HUGGINGFACE_API_KEY'
    ]
    
    missing_vars = []
    for var in required_env_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print(f"âš ï¸  è­¦å‘Š: ä»¥ä¸‹ç¯å¢ƒå˜é‡æœªè®¾ç½®: {missing_vars}")
        print("ç³»ç»Ÿå°†ä½¿ç”¨å¤‡ç”¨æ¨¡å¼è¿è¡Œ")
    
    print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")

def run_data_preprocessing(data_path: str, sample_size: int = 500):
    """è¿è¡Œæ•°æ®é¢„å¤„ç†"""
    print("\nğŸ“‹ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    
    try:
        from custom_pipeline.data_processor import DatasetProcessor
        
        processor = DatasetProcessor(data_path)
        
        # å‡†å¤‡æ•°æ®
        if data_path.endswith('.jsonl'):
            documents = processor.prepare_for_topicgpt(
                max_docs=1000,
                sample_size=sample_size
            )
        else:
            documents = processor.prepare_for_topicgpt(
                text_column="Translated Post Description",
                max_docs=1000,
                sample_size=sample_size
            )
        
        # ä¿å­˜æ•°æ®
        processor.save_to_jsonl(documents, "data/input/dataset.jsonl")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = processor.get_data_statistics()
        
        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"å¤„ç†æ–‡æ¡£æ•°: {stats['total_documents']}")
        print(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {stats['avg_text_length']:.1f}")
        print(f"æ•°æ®æºç±»å‹: {stats['source_type']}")
        
        return documents, stats
        
    except Exception as e:
        print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
        raise

def run_topic_modeling(documents, config: Dict[str, Any] = None):
    """è¿è¡Œè¯é¢˜å»ºæ¨¡"""
    print("\nğŸš€ å¼€å§‹è¯é¢˜å»ºæ¨¡...")
    
    try:
        from custom_pipeline.topicgpt_runner import CustomTopicGPTRunner
        
        runner = CustomTopicGPTRunner(config)
        results = runner.run_topic_modeling(documents)
        
        print("âœ… è¯é¢˜å»ºæ¨¡å®Œæˆ")
        return results
        
    except Exception as e:
        print(f"âŒ è¯é¢˜å»ºæ¨¡å¤±è´¥: {e}")
        raise

def run_evaluation(topicgpt_results_path: str, original_docs_path: str, config: Dict[str, Any] = None):
    """è¿è¡Œè´¨é‡è¯„ä¼°"""
    print("\nğŸ“Š å¼€å§‹è´¨é‡è¯„ä¼°...")
    
    try:
        from custom_pipeline.geval_runner import CustomGEvalRunner
        
        geval_runner = CustomGEvalRunner(config)
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        geval_input = geval_runner.prepare_geval_input(topicgpt_results_path, original_docs_path)
        
        # è¿è¡Œè¯„ä¼°
        eval_results = geval_runner.run_evaluation(geval_input)
        
        # ä¿å­˜ç»“æœ
        geval_runner.save_evaluation_results(eval_results, 'results/geval_results.json')
        
        # ç”ŸæˆæŠ¥å‘Š
        report = geval_runner.generate_evaluation_report(eval_results)
        with open('results/evaluation_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("âœ… è´¨é‡è¯„ä¼°å®Œæˆ")
        print(f"æ€»ä½“è¯„åˆ†: {eval_results['overall_score']:.2f}/5.00")
        
        return eval_results
        
    except Exception as e:
        print(f"âŒ è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
        raise

def run_closed_loop_system(data_path: str, config: Dict[str, Any] = None):
    """è¿è¡Œå®Œæ•´çš„é—­ç¯ç³»ç»Ÿ"""
    print("\nğŸ¯ å¼€å§‹é—­ç¯è¯é¢˜å»ºæ¨¡ç³»ç»Ÿ...")
    
    try:
        from custom_pipeline.feedback_loop import ClosedLoopTopicModeling
        
        pipeline = ClosedLoopTopicModeling(data_path, config)
        
        # è¿è¡Œå®Œæ•´æµç¨‹
        results = pipeline.run_complete_pipeline(
            max_iterations=config.get('max_iterations', 3),
            sample_size=config.get('sample_size', 500)
        )
        
        print("âœ… é—­ç¯ç³»ç»Ÿè¿è¡Œå®Œæˆ")
        return results
        
    except Exception as e:
        print(f"âŒ é—­ç¯ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
        raise

def create_config(args) -> Dict[str, Any]:
    """åˆ›å»ºé…ç½®"""
    config = {
        'api_type': args.api_type,
        'model_name': args.model_name,
        'num_topics': args.num_topics,
        'max_iterations': args.max_iterations,
        'sample_size': args.sample_size,
        'temperature': args.temperature,
        'max_tokens': args.max_tokens,
        'optimization_threshold': args.optimization_threshold,
        'evaluation_aspects': [
            'coherence',
            'consistency', 
            'fluency',
            'relevance',
            'diversity'
        ]
    }
    
    return config

def save_config(config: Dict[str, Any], output_path: str):
    """ä¿å­˜é…ç½®"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    print(f"é…ç½®å·²ä¿å­˜åˆ°: {output_path}")

def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®"""
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŸºäºLLMçš„è‡ªåŠ¨è¯é¢˜å»ºæ¨¡+è¯„ä¼°+é—­ç¯ä¼˜åŒ–ç³»ç»Ÿ')
    
    parser.add_argument('--data_path', type=str, default='mydata.jsonl',
                       help='æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ.jsonlæˆ–.xlsxæ ¼å¼ï¼‰')
    parser.add_argument('--mode', type=str, default='closed_loop',
                       choices=['preprocess', 'topic_modeling', 'evaluation', 'closed_loop'],
                       help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--api_type', type=str, default='openai',
                       choices=['openai', 'huggingface', 'local'],
                       help='APIç±»å‹')
    parser.add_argument('--model_name', type=str, default='gpt-3.5-turbo',
                       help='æ¨¡å‹åç§°')
    parser.add_argument('--num_topics', type=int, default=8,
                       help='è¯é¢˜æ•°é‡')
    parser.add_argument('--max_iterations', type=int, default=3,
                       help='æœ€å¤§ä¼˜åŒ–è½®æ•°')
    parser.add_argument('--sample_size', type=int, default=500,
                       help='é‡‡æ ·å¤§å°')
    parser.add_argument('--temperature', type=float, default=0.7,
                       help='æ¸©åº¦å‚æ•°')
    parser.add_argument('--max_tokens', type=int, default=1000,
                       help='æœ€å¤§tokenæ•°')
    parser.add_argument('--optimization_threshold', type=float, default=3.0,
                       help='ä¼˜åŒ–é˜ˆå€¼')
    parser.add_argument('--config_path', type=str, default='config.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='results',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.data_path):
        print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ '{args.data_path}' ä¸å­˜åœ¨")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ï¼Œæˆ–ä½¿ç”¨ --data_path å‚æ•°æŒ‡å®šæ­£ç¡®çš„æ–‡ä»¶è·¯å¾„")
        return 1
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # åˆ›å»ºé…ç½®
    config = create_config(args)
    
    # ä¿å­˜é…ç½®
    save_config(config, 'config.json')
    
    try:
        if args.mode == 'preprocess':
            # ä»…è¿è¡Œæ•°æ®é¢„å¤„ç†
            documents, stats = run_data_preprocessing(args.data_path, args.sample_size)
            print(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œç»Ÿè®¡ä¿¡æ¯: {stats}")
            
        elif args.mode == 'topic_modeling':
            # ä»…è¿è¡Œè¯é¢˜å»ºæ¨¡
            documents, _ = run_data_preprocessing(args.data_path, args.sample_size)
            results = run_topic_modeling(documents, config)
            print(f"è¯é¢˜å»ºæ¨¡å®Œæˆï¼Œç”Ÿæˆäº† {len(results.get('topics', {}))} ä¸ªè¯é¢˜")
            
        elif args.mode == 'evaluation':
            # ä»…è¿è¡Œè¯„ä¼°
            if not os.path.exists('data/output/topicgpt_results/topicgpt_results.json'):
                print("âŒ æœªæ‰¾åˆ°è¯é¢˜å»ºæ¨¡ç»“æœï¼Œè¯·å…ˆè¿è¡Œè¯é¢˜å»ºæ¨¡")
                return 1
            
            eval_results = run_evaluation(
                'data/output/topicgpt_results/topicgpt_results.json',
                'data/input/dataset.jsonl',
                config
            )
            print(f"è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {eval_results['overall_score']:.2f}")
            
        elif args.mode == 'closed_loop':
            # è¿è¡Œå®Œæ•´é—­ç¯ç³»ç»Ÿ
            results = run_closed_loop_system(args.data_path, config)
            print(f"é—­ç¯ç³»ç»Ÿå®Œæˆï¼Œæœ€ä½³åˆ†æ•°: {results.get('evaluation', {}).get('overall_score', 0):.2f}")
        
        print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 