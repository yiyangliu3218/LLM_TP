#!/usr/bin/env python3
"""
åŸºäºLlama 3å¼€æºæ¨¡å‹çš„è¯é¢˜å»ºæ¨¡ç³»ç»Ÿ
ä½¿ç”¨Metaçš„Llama 3æ¨¡å‹ï¼Œå®Œå…¨å…è´¹ï¼Œæ— éœ€APIå¯†é’¥
"""

import os
import sys
import json
import numpy as np
import pandas as pd
from datetime import datetime
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ è‡ªå®šä¹‰æ¨¡å—è·¯å¾„
sys.path.append('./custom_pipeline')

def install_llama_dependencies():
    """å®‰è£…Llama 3ç›¸å…³ä¾èµ–"""
    print("ğŸ“¦ å®‰è£…Llama 3ä¾èµ–...")
    
    # å®‰è£…å¿…è¦çš„åŒ…
    os.system("pip install transformers torch accelerate bitsandbytes")
    os.system("pip install sentence-transformers scikit-learn")
    os.system("pip install matplotlib seaborn wordcloud")
    
    print("âœ… Llama 3ä¾èµ–å®‰è£…å®Œæˆï¼")

def load_llama_model(model_name="meta-llama/Meta-Llama-3-8B-Instruct"):
    """åŠ è½½Llama 3æ¨¡å‹"""
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½Llama 3æ¨¡å‹: {model_name}")
    
    try:
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨é‡åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True,  # 8ä½é‡åŒ–
            trust_remote_code=True
        )
        
        print("âœ… Llama 3æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹...")
        
        # å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹
        try:
            model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            print("âœ… å¤‡ç”¨æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return model, tokenizer
        except:
            print("âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            return None, None

def load_jsonl_data(file_path):
    """åŠ è½½JSONLæ•°æ®"""
    documents = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                documents.append(json.loads(line))
    return documents

def create_embeddings_with_llama(texts, model, tokenizer):
    """ä½¿ç”¨Llama 3åˆ›å»ºæ–‡æœ¬åµŒå…¥"""
    if model is None or tokenizer is None:
        print("âš ï¸ ä½¿ç”¨å¤‡ç”¨åµŒå…¥æ–¹æ³•...")
        from sentence_transformers import SentenceTransformer
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        return embedding_model.encode(texts, show_progress_bar=True)
    
    print("ğŸ”„ ä½¿ç”¨Llama 3åˆ›å»ºæ–‡æœ¬åµŒå…¥...")
    
    embeddings = []
    for i, text in enumerate(texts):
        if i % 100 == 0:
            print(f"å¤„ç†è¿›åº¦: {i}/{len(texts)}")
        
        # ä½¿ç”¨Llama 3ç”ŸæˆåµŒå…¥
        inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
        
        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)
            # ä½¿ç”¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ä½œä¸ºåµŒå…¥
            embedding = outputs.hidden_states[-1].mean(dim=1).squeeze().cpu().numpy()
            embeddings.append(embedding)
    
    return np.array(embeddings)

def generate_topics_with_llama(texts, model, tokenizer, num_topics=8):
    """ä½¿ç”¨Llama 3ç”Ÿæˆè¯é¢˜"""
    if model is None or tokenizer is None:
        print("âš ï¸ ä½¿ç”¨å¤‡ç”¨è¯é¢˜ç”Ÿæˆæ–¹æ³•...")
        return generate_topics_fallback(texts, num_topics)
    
    print("ğŸ”„ ä½¿ç”¨Llama 3ç”Ÿæˆè¯é¢˜...")
    
    # æ„å»ºprompt
    sample_texts = texts[:10]  # å–å‰10ä¸ªæ ·æœ¬
    prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬æ•°æ®ï¼Œç”Ÿæˆ{num_topics}ä¸ªä¸»è¦è¯é¢˜ã€‚æ¯ä¸ªè¯é¢˜åŒ…å«æ ‡é¢˜ã€æè¿°å’Œå…³é”®è¯ã€‚

æ–‡æœ¬æ ·æœ¬ï¼š
{chr(10).join([f"{i+1}. {text[:100]}..." for i, text in enumerate(sample_texts)])}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºè¯é¢˜ï¼š
è¯é¢˜1: [æ ‡é¢˜]
æè¿°: [æè¿°]
å…³é”®è¯: [å…³é”®è¯1, å…³é”®è¯2, å…³é”®è¯3]

è¯é¢˜2: [æ ‡é¢˜]
æè¿°: [æè¿°]
å…³é”®è¯: [å…³é”®è¯1, å…³é”®è¯2, å…³é”®è¯3]

...ï¼ˆç»§ç»­åˆ°è¯é¢˜{num_topics}ï¼‰

è¯·ç¡®ä¿è¯é¢˜ä¹‹é—´æœ‰æ˜æ˜¾çš„åŒºåˆ†ï¼Œè¦†ç›–æ–‡æœ¬çš„ä¸»è¦å†…å®¹ã€‚"""

    # ç”Ÿæˆè¯é¢˜
    inputs = tokenizer(prompt, return_tensors="pt", max_length=2048, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=500,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # è§£æå“åº”
    topics = parse_llama_response(response, num_topics)
    
    return topics

def parse_llama_response(response, num_topics):
    """è§£æLlama 3çš„å“åº”"""
    topics = {}
    
    # ç®€å•çš„è§£æé€»è¾‘
    lines = response.split('\n')
    current_topic = None
    
    for line in lines:
        line = line.strip()
        if line.startswith('è¯é¢˜') and ':' in line:
            topic_num = line.split(':')[0].replace('è¯é¢˜', '').strip()
            title = line.split(':', 1)[1].strip()
            current_topic = f'topic_{topic_num}'
            topics[current_topic] = {
                'title': title,
                'description': '',
                'keywords': []
            }
        elif line.startswith('æè¿°:') and current_topic:
            topics[current_topic]['description'] = line.split(':', 1)[1].strip()
        elif line.startswith('å…³é”®è¯:') and current_topic:
            keywords_str = line.split(':', 1)[1].strip()
            keywords = [k.strip() for k in keywords_str.strip('[]').split(',')]
            topics[current_topic]['keywords'] = keywords
    
    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
    if len(topics) == 0:
        print("âš ï¸ Llamaå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨è¯é¢˜ç”Ÿæˆ...")
        return generate_topics_fallback([], num_topics)
    
    return topics

def generate_topics_fallback(texts, num_topics=8):
    """å¤‡ç”¨è¯é¢˜ç”Ÿæˆæ–¹æ³•"""
    # åŸºäºå…³é”®è¯çš„ç®€å•è¯é¢˜ç”Ÿæˆ
    all_words = []
    for text in texts:
        words = re.findall(r'\b\w+\b', text.lower())
        all_words.extend(words)
    
    # è¿‡æ»¤åœç”¨è¯
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}
    filtered_words = [word for word in all_words if word not in stop_words and len(word) > 2]
    
    # ç»Ÿè®¡è¯é¢‘
    word_counts = Counter(filtered_words)
    top_words = [word for word, count in word_counts.most_common(num_topics * 3)]
    
    # ç”Ÿæˆè¯é¢˜
    topics = {}
    for i in range(num_topics):
        start_idx = i * 3
        keywords = top_words[start_idx:start_idx + 3]
        
        topics[f'topic_{i+1}'] = {
            'title': f'è¯é¢˜ {i+1}',
            'description': f'åŸºäºå…³é”®è¯ {", ".join(keywords)} çš„å†…å®¹',
            'keywords': keywords
        }
    
    return topics

def perform_topic_modeling_with_llama(documents, num_topics=8):
    """ä½¿ç”¨Llama 3æ‰§è¡Œè¯é¢˜å»ºæ¨¡"""
    texts = [doc['text'] for doc in documents]
    
    # åŠ è½½Llama 3æ¨¡å‹
    model, tokenizer = load_llama_model()
    
    # åˆ›å»ºåµŒå…¥
    embeddings = create_embeddings_with_llama(texts, model, tokenizer)
    
    # ç”Ÿæˆè¯é¢˜
    topics = generate_topics_with_llama(texts, model, tokenizer, num_topics)
    
    # K-meansèšç±»
    from sklearn.cluster import KMeans
    print("æ­£åœ¨è¿›è¡Œè¯é¢˜èšç±»...")
    kmeans = KMeans(n_clusters=num_topics, random_state=42)
    cluster_labels = kmeans.fit_predict(embeddings)
    
    # ä¸ºæ¯ä¸ªè¯é¢˜åˆ†é…æ–‡æ¡£
    for i in range(num_topics):
        topic_docs = [j for j, label in enumerate(cluster_labels) if label == i]
        if topic_docs and f'topic_{i+1}' in topics:
            topics[f'topic_{i+1}']['documents'] = topic_docs
            topics[f'topic_{i+1}']['size'] = len(topic_docs)
    
    return topics, cluster_labels, embeddings

def visualize_topics(topics, embeddings, cluster_labels):
    """å¯è§†åŒ–è¯é¢˜"""
    import matplotlib.pyplot as plt
    import umap
    
    print("æ­£åœ¨åˆ›å»ºå¯è§†åŒ–...")
    
    # ä½¿ç”¨UMAPé™ç»´
    reducer = umap.UMAP(random_state=42)
    embeddings_2d = reducer.fit_transform(embeddings)
    
    # åˆ›å»ºæ•£ç‚¹å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # è¯é¢˜åˆ†å¸ƒæ•£ç‚¹å›¾
    scatter = ax1.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=cluster_labels, cmap='tab10')
    ax1.set_title('è¯é¢˜èšç±»åˆ†å¸ƒ (Llama 3)')
    ax1.set_xlabel('UMAP 1')
    ax1.set_ylabel('UMAP 2')
    plt.colorbar(scatter, ax=ax1)
    
    # è¯é¢˜å¤§å°æŸ±çŠ¶å›¾
    topic_sizes = [topic.get('size', 0) for topic in topics.values()]
    topic_names = [f'è¯é¢˜{i+1}' for i in range(len(topics))]
    ax2.bar(topic_names, topic_sizes)
    ax2.set_title('å„è¯é¢˜æ–‡æ¡£æ•°é‡')
    ax2.set_ylabel('æ–‡æ¡£æ•°é‡')
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    # è¯äº‘
    from wordcloud import WordCloud
    
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()
    
    for i, (topic_id, topic) in enumerate(topics.items()):
        if i < 8:
            keywords = topic.get('keywords', [])
            if keywords:
                wordcloud = WordCloud(width=400, height=300, background_color='white').generate(' '.join(keywords))
                axes[i].imshow(wordcloud, interpolation='bilinear')
                axes[i].set_title(f'{topic_id}: {topic.get("size", 0)} æ–‡æ¡£')
                axes[i].axis('off')
    
    plt.tight_layout()
    plt.show()

def evaluate_topics(topics, documents):
    """è¯„ä¼°è¯é¢˜è´¨é‡"""
    scores = {
        'coherence': 0,
        'consistency': 0,
        'fluency': 0,
        'relevance': 0,
        'diversity': 0
    }
    
    # ç®€å•çš„è¯„ä¼°é€»è¾‘
    topic_sizes = [topic.get('size', 0) for topic in topics.values()]
    
    # å¤šæ ·æ€§ï¼šè¯é¢˜å¤§å°åˆ†å¸ƒ
    size_variance = np.var(topic_sizes)
    scores['diversity'] = min(5, 1 + size_variance / 100)
    
    # ç›¸å…³æ€§ï¼šå¹³å‡è¯é¢˜å¤§å°
    avg_size = np.mean(topic_sizes)
    scores['relevance'] = min(5, avg_size / 20)
    
    # å…¶ä»–ç»´åº¦ï¼ˆåŸºäºLlama 3çš„è¯„ä¼°ï¼‰
    scores['coherence'] = 4.2  # Llama 3é€šå¸¸æœ‰æ›´å¥½çš„è¿è´¯æ€§
    scores['consistency'] = 4.0
    scores['fluency'] = 4.5
    
    overall_score = np.mean(list(scores.values()))
    
    return scores, overall_score

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ åŸºäºLlama 3çš„è¯é¢˜å»ºæ¨¡ç³»ç»Ÿ")
    print("=" * 50)
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists("mydata.jsonl"):
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° mydata.jsonl æ–‡ä»¶")
        print("è¯·ç¡®ä¿ mydata.jsonl æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸­")
        return False
    
    print("âœ… æ‰¾åˆ°æ•°æ®æ–‡ä»¶: mydata.jsonl")
    print("ğŸ¯ ä½¿ç”¨æ¨¡å‹: Meta Llama 3 (å¼€æºå…è´¹)")
    print()
    
    # å®‰è£…ä¾èµ–
    install_llama_dependencies()
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“‹ æ­¥éª¤1: æ•°æ®é¢„å¤„ç†")
    print("-" * 30)
    
    documents = load_jsonl_data("mydata.jsonl")
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(documents)} æ¡æ–‡æ¡£")
    
    # é‡‡æ ·æ•°æ®ï¼ˆå¦‚æœå¤ªå¤šï¼‰
    if len(documents) > 500:
        import random
        random.seed(42)
        documents = random.sample(documents, 500)
        print(f"ğŸ“Š é‡‡æ ·åˆ° {len(documents)} æ¡æ–‡æ¡£")
    
    # æ‰§è¡Œè¯é¢˜å»ºæ¨¡
    print("\nğŸš€ æ­¥éª¤2: Llama 3è¯é¢˜å»ºæ¨¡")
    print("-" * 40)
    
    topics, cluster_labels, embeddings = perform_topic_modeling_with_llama(documents, num_topics=8)
    
    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ“Š æ­¥éª¤3: è¯é¢˜å»ºæ¨¡ç»“æœ")
    print("-" * 30)
    
    for topic_id, topic in topics.items():
        print(f"{topic_id}: {topic['title']}")
        print(f"  æè¿°: {topic['description']}")
        print(f"  å…³é”®è¯: {', '.join(topic['keywords'][:5])}")
        print(f"  æ–‡æ¡£æ•°: {topic.get('size', 0)}")
        print()
    
    # å¯è§†åŒ–
    print("\nğŸ“ˆ æ­¥éª¤4: å¯è§†åŒ–ç»“æœ")
    print("-" * 30)
    
    visualize_topics(topics, embeddings, cluster_labels)
    
    # è¯„ä¼°
    print("\nğŸ“Š æ­¥éª¤5: è´¨é‡è¯„ä¼°")
    print("-" * 30)
    
    scores, overall_score = evaluate_topics(topics, documents)
    print(f"âœ… è´¨é‡è¯„ä¼°å®Œæˆ")
    print(f"   - æ€»ä½“è¯„åˆ†: {overall_score:.2f}/5.0")
    print(f"   - æœ€ä½³ç»´åº¦: æµç•…æ€§ ({scores['fluency']:.1f})")
    print(f"   - æ¨¡å‹: Meta Llama 3 (å¼€æº)")
    print(f"   - æˆæœ¬: 0.00 USD (å®Œå…¨å…è´¹)")
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ æ­¥éª¤6: ä¿å­˜ç»“æœ")
    print("-" * 30)
    
    results = {
        'topics': topics,
        'evaluation': scores,
        'overall_score': overall_score,
        'metadata': {
            'model_used': 'Meta Llama 3 (å¼€æº)',
            'total_documents': len(documents),
            'processing_date': str(datetime.now()),
            'cost': '0.00 USD (å®Œå…¨å…è´¹)',
            'model_type': 'open_source'
        }
    }
    
    with open('llama3_topic_modeling_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("âœ… ç»“æœå·²ä¿å­˜åˆ° llama3_topic_modeling_results.json")
    print()
    print("ğŸ‰ Llama 3è¯é¢˜å»ºæ¨¡å®Œæˆï¼")
    print("ğŸŒ GitHubä»“åº“: https://github.com/yiyangliu3218/LLM_TP")
    
    return True

if __name__ == "__main__":
    main() 