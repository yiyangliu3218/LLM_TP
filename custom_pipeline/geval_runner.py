import sys
import os
import json
import requests
from typing import Dict, List, Any
from datetime import datetime

# æ·»åŠ G-Evalè·¯å¾„
sys.path.append('./geval')

class CustomGEvalRunner:
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–G-Evalè¿è¡Œå™¨"""
        self.config = config or self.get_default_config()
        self.evaluation_results = {}
        
    def get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'api_type': 'openai',  # æˆ– 'huggingface', 'local'
            'model_name': 'gpt-3.5-turbo',
            'evaluation_aspects': [
                'coherence',      # è¯é¢˜è¿è´¯æ€§
                'consistency',    # è¯é¢˜ä¸€è‡´æ€§  
                'fluency',        # æµç•…æ€§
                'relevance',      # ç›¸å…³æ€§
                'diversity'       # å¤šæ ·æ€§
            ],
            'scoring_scale': 5,   # è¯„åˆ†èŒƒå›´1-5
            'model_config': {
                'openai': {
                    'api_key': os.getenv('OPENAI_API_KEY', ''),
                    'base_url': os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
                },
                'huggingface': {
                    'model_name': 'microsoft/DialoGPT-medium',
                    'api_key': os.getenv('HUGGINGFACE_API_KEY', '')
                }
            }
        }
    
    def prepare_geval_input(self, topicgpt_results_path: str, 
                           original_docs_path: str) -> Dict[str, Any]:
        """å°†TopicGPTç»“æœè½¬æ¢ä¸ºG-Evalè¾“å…¥æ ¼å¼"""
        
        print("ğŸ“‹ å‡†å¤‡G-Evalè¾“å…¥æ•°æ®...")
        
        # è¯»å–TopicGPTç»“æœ
        with open(topicgpt_results_path, 'r', encoding='utf-8') as f:
            topic_results = json.load(f)
        
        # è¯»å–åŸå§‹æ–‡æ¡£
        with open(original_docs_path, 'r', encoding='utf-8') as f:
            original_docs = [json.loads(line) for line in f]
        
        # æ„å»ºG-Evalè¾“å…¥æ ¼å¼
        geval_input = {
            "task_description": "è¯„ä¼°è¯é¢˜å»ºæ¨¡ç»“æœçš„è´¨é‡",
            "topics": topic_results.get('topics', {}),
            "assignments": topic_results.get('assignments', {}),
            "documents": original_docs,
            "evaluation_aspects": self.config['evaluation_aspects'],
            "metadata": topic_results.get('metadata', {})
        }
        
        print(f"âœ… G-Evalè¾“å…¥å‡†å¤‡å®Œæˆï¼ŒåŒ…å« {len(geval_input['topics'])} ä¸ªè¯é¢˜å’Œ {len(original_docs)} ä¸ªæ–‡æ¡£")
        return geval_input
    
    def run_evaluation(self, geval_input: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡ŒG-Evalè¯„ä¼°"""
        print("ğŸ“Š å¼€å§‹G-Evalè¯„ä¼°...")
        
        evaluation_results = {}
        
        for aspect in geval_input['evaluation_aspects']:
            print(f"è¯„ä¼°ç»´åº¦: {aspect}")
            
            # æ„å»ºè¯„ä¼°prompt
            eval_prompt = self.build_evaluation_prompt(
                aspect, 
                geval_input['topics'], 
                geval_input['assignments'],
                geval_input['documents']
            )
            
            # è°ƒç”¨æ¨¡å‹è¿›è¡Œè¯„ä¼°
            score, reasoning = self.evaluate_aspect(eval_prompt, aspect)
            
            evaluation_results[aspect] = {
                "score": score,
                "reasoning": reasoning,
                "prompt": eval_prompt
            }
        
        # è®¡ç®—æ€»åˆ†å’ŒåŠ æƒåˆ†æ•°
        overall_score = self.calculate_overall_score(evaluation_results)
        evaluation_results['overall_score'] = overall_score
        
        # æ·»åŠ è¯„ä¼°å…ƒæ•°æ®
        evaluation_results['metadata'] = {
            'evaluation_date': datetime.now().isoformat(),
            'model_used': self.config['model_name'],
            'scoring_scale': self.config['scoring_scale'],
            'total_aspects': len(geval_input['evaluation_aspects'])
        }
        
        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {overall_score:.2f}")
        return evaluation_results
    
    def evaluate_aspect(self, prompt: str, aspect: str) -> tuple:
        """è¯„ä¼°ç‰¹å®šç»´åº¦"""
        try:
            response = self.call_model(prompt)
            score, reasoning = self.parse_evaluation_response(response, aspect)
            return score, reasoning
        except Exception as e:
            print(f"è¯„ä¼°ç»´åº¦ {aspect} æ—¶å‡ºé”™: {e}")
            return 3.0, f"è¯„ä¼°å¤±è´¥: {str(e)}"
    
    def call_model(self, prompt: str) -> str:
        """è°ƒç”¨æ¨¡å‹"""
        api_type = self.config['api_type']
        
        if api_type == 'openai':
            return self.call_openai_model(prompt)
        elif api_type == 'huggingface':
            return self.call_huggingface_model(prompt)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„APIç±»å‹: {api_type}")
    
    def call_openai_model(self, prompt: str) -> str:
        """è°ƒç”¨OpenAIæ¨¡å‹"""
        try:
            import openai
            openai.api_key = self.config['model_config']['openai']['api_key']
            openai.base_url = self.config['model_config']['openai']['base_url']
            
            response = openai.ChatCompletion.create(
                model=self.config['model_name'],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯é¢˜å»ºæ¨¡è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            return self.get_fallback_response()
    
    def call_huggingface_model(self, prompt: str) -> str:
        """è°ƒç”¨HuggingFaceæ¨¡å‹"""
        try:
            headers = {
                "Authorization": f"Bearer {self.config['model_config']['huggingface']['api_key']}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_new_tokens": 1000,
                    "temperature": 0.3
                }
            }
            
            response = requests.post(
                f"https://api-inference.huggingface.co/models/{self.config['model_config']['huggingface']['model_name']}",
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                return response.json()[0]['generated_text']
            else:
                print(f"HuggingFace APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                return self.get_fallback_response()
                
        except Exception as e:
            print(f"HuggingFace APIè°ƒç”¨å¤±è´¥: {e}")
            return self.get_fallback_response()
    
    def get_fallback_response(self) -> str:
        """è·å–å¤‡ç”¨å“åº”"""
        return "è¯„åˆ†: 3.0\nç†ç”±: ç”±äºæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œç»™å‡ºä¸­ç­‰è¯„åˆ†ã€‚"
    
    def build_evaluation_prompt(self, aspect: str, topics: Dict[str, Any], 
                               assignments: Dict[str, Any], documents: List[Dict[str, Any]]) -> str:
        """æ„å»ºç‰¹å®šç»´åº¦çš„è¯„ä¼°prompt"""
        
        aspect_prompts = {
            "coherence": """
            è¯„ä¼°ä»¥ä¸‹è¯é¢˜å»ºæ¨¡ç»“æœä¸­æ¯ä¸ªè¯é¢˜çš„å†…éƒ¨è¿è´¯æ€§ã€‚
            
            è¯„ä¼°æ ‡å‡†ï¼š
            1. è¯é¢˜å†…çš„å…³é”®è¯æ˜¯å¦è¯­ä¹‰ç›¸å…³
            2. è¯é¢˜æè¿°æ˜¯å¦ä¸å…³é”®è¯ä¸€è‡´
            3. åˆ†é…åˆ°è¯¥è¯é¢˜çš„æ–‡æ¡£æ˜¯å¦ç¬¦åˆè¯é¢˜ä¸»é¢˜
            4. è¯é¢˜å†…éƒ¨é€»è¾‘æ˜¯å¦æ¸…æ™°
            
            è¯·å¯¹æ¯ä¸ªè¯é¢˜çš„è¿è´¯æ€§è¿›è¡Œ1-5åˆ†è¯„åˆ†ï¼Œå¹¶ç»™å‡ºè¯¦ç»†ç†ç”±ã€‚
            """,
            
            "consistency": """
            è¯„ä¼°è¯é¢˜å»ºæ¨¡ç»“æœçš„æ•´ä½“ä¸€è‡´æ€§ã€‚
            
            è¯„ä¼°æ ‡å‡†ï¼š
            1. ä¸åŒè¯é¢˜ä¹‹é—´æ˜¯å¦æœ‰æ˜ç¡®åŒºåˆ†
            2. è¯é¢˜æ ‡ç­¾å’Œæè¿°çš„å‘½åæ˜¯å¦ä¸€è‡´
            3. è¯é¢˜åˆ†é…æ˜¯å¦åˆç†æ— é‡å 
            4. æ•´ä½“è¯é¢˜ä½“ç³»æ˜¯å¦åè°ƒ
            
            è¯·ç»™å‡º1-5åˆ†çš„æ•´ä½“ä¸€è‡´æ€§è¯„åˆ†å’Œè¯¦ç»†ç†ç”±ã€‚
            """,
            
            "fluency": """
            è¯„ä¼°è¯é¢˜å»ºæ¨¡ç»“æœçš„æµç•…æ€§ã€‚
            
            è¯„ä¼°æ ‡å‡†ï¼š
            1. è¯é¢˜æè¿°æ˜¯å¦è‡ªç„¶æµç•…
            2. å…³é”®è¯è¡¨è¾¾æ˜¯å¦å‡†ç¡®
            3. è¯é¢˜æ ‡é¢˜æ˜¯å¦ç®€æ´æ˜äº†
            4. æ•´ä½“è¡¨è¾¾æ˜¯å¦æ˜“äºç†è§£
            
            è¯·ç»™å‡º1-5åˆ†çš„æµç•…æ€§è¯„åˆ†å’Œè¯¦ç»†ç†ç”±ã€‚
            """,
            
            "relevance": """
            è¯„ä¼°è¯é¢˜å»ºæ¨¡ç»“æœä¸åŸå§‹æ–‡æ¡£çš„ç›¸å…³æ€§ã€‚
            
            è¯„ä¼°æ ‡å‡†ï¼š
            1. è¯é¢˜æ˜¯å¦å‡†ç¡®åæ˜ äº†æ–‡æ¡£å†…å®¹
            2. è¯é¢˜åˆ†é…æ˜¯å¦åˆç†
            3. è¯é¢˜è¦†ç›–é¢æ˜¯å¦å…¨é¢
            4. æ˜¯å¦é—æ¼äº†é‡è¦ä¸»é¢˜
            
            è¯·ç»™å‡º1-5åˆ†çš„ç›¸å…³æ€§è¯„åˆ†å’Œè¯¦ç»†ç†ç”±ã€‚
            """,
            
            "diversity": """
            è¯„ä¼°è¯é¢˜å»ºæ¨¡ç»“æœçš„å¤šæ ·æ€§ã€‚
            
            è¯„ä¼°æ ‡å‡†ï¼š
            1. è¯é¢˜æ˜¯å¦æ¶µç›–äº†ä¸åŒçš„ä¸»é¢˜é¢†åŸŸ
            2. è¯é¢˜ä¹‹é—´æ˜¯å¦æœ‰è¶³å¤Ÿçš„å·®å¼‚æ€§
            3. æ˜¯å¦é¿å…äº†è¯é¢˜é‡å¤
            4. è¯é¢˜åˆ†å¸ƒæ˜¯å¦å‡è¡¡
            
            è¯·ç»™å‡º1-5åˆ†çš„å¤šæ ·æ€§è¯„åˆ†å’Œè¯¦ç»†ç†ç”±ã€‚
            """
        }
        
        base_prompt = aspect_prompts.get(aspect, "è¯·è¯„ä¼°è¯é¢˜å»ºæ¨¡ç»“æœçš„è´¨é‡")
        
        # å‡†å¤‡è¯é¢˜ä¿¡æ¯
        topics_info = []
        for topic_id, topic_data in topics.items():
            if isinstance(topic_data, dict):
                topic_info = {
                    "id": topic_id,
                    "title": topic_data.get('title', f'Topic {topic_id}'),
                    "description": topic_data.get('description', ''),
                    "keywords": topic_data.get('keywords', [])
                }
                topics_info.append(topic_info)
        
        # å‡†å¤‡æ–‡æ¡£æ ·æœ¬
        sample_docs = documents[:5] if len(documents) > 5 else documents
        
        prompt = f"""
        {base_prompt}
        
        è¯é¢˜å»ºæ¨¡ç»“æœï¼š
        {json.dumps(topics_info, ensure_ascii=False, indent=2)}
        
        è¯é¢˜åˆ†é…ç»Ÿè®¡ï¼š
        {self.get_assignment_statistics(assignments)}
        
        åŸå§‹æ–‡æ¡£æ ·æœ¬ï¼š
        {json.dumps([{"id": doc['id'], "text": doc['text'][:200] + "..." if len(doc['text']) > 200 else doc['text']} for doc in sample_docs], ensure_ascii=False, indent=2)}
        
        è¯·æŒ‰ç…§æ ‡å‡†è¿›è¡Œè¯„ä¼°å¹¶ç»™å‡ºåˆ†æ•°(1-5)å’Œè¯¦ç»†ç†ç”±ã€‚
        æ ¼å¼ï¼š
        è¯„åˆ†: [åˆ†æ•°]
        ç†ç”±: [è¯¦ç»†ç†ç”±]
        """
        
        return prompt
    
    def get_assignment_statistics(self, assignments: Dict[str, Any]) -> Dict[str, int]:
        """è·å–è¯é¢˜åˆ†é…ç»Ÿè®¡"""
        stats = {}
        for doc_id, assignment in assignments.items():
            topic_id = assignment.get('topic_id', 'unknown')
            stats[topic_id] = stats.get(topic_id, 0) + 1
        return stats
    
    def parse_evaluation_response(self, response: str, aspect: str) -> tuple:
        """è§£æè¯„ä¼°å“åº”"""
        try:
            # å°è¯•æå–åˆ†æ•°
            score = 3.0  # é»˜è®¤åˆ†æ•°
            reasoning = response
            
            # æŸ¥æ‰¾åˆ†æ•°
            if "è¯„åˆ†:" in response:
                score_part = response.split("è¯„åˆ†:")[1].split("\n")[0].strip()
                try:
                    score = float(score_part)
                    # ç¡®ä¿åˆ†æ•°åœ¨1-5èŒƒå›´å†…
                    score = max(1.0, min(5.0, score))
                except:
                    pass
            
            # æŸ¥æ‰¾ç†ç”±
            if "ç†ç”±:" in response:
                reasoning = response.split("ç†ç”±:")[1].strip()
            
            return score, reasoning
            
        except Exception as e:
            print(f"è§£æè¯„ä¼°å“åº”å¤±è´¥: {e}")
            return 3.0, f"è§£æå¤±è´¥: {response}"
    
    def calculate_overall_score(self, evaluation_results: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»åˆ†"""
        scores = []
        weights = {
            'coherence': 0.25,
            'consistency': 0.25,
            'fluency': 0.15,
            'relevance': 0.25,
            'diversity': 0.10
        }
        
        for aspect, result in evaluation_results.items():
            if aspect != 'overall_score' and aspect != 'metadata':
                score = result.get('score', 3.0)
                weight = weights.get(aspect, 1.0 / len(evaluation_results))
                scores.append(score * weight)
        
        if scores:
            return sum(scores)
        else:
            return 3.0
    
    def save_evaluation_results(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    def generate_evaluation_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = []
        report.append("# è¯é¢˜å»ºæ¨¡è´¨é‡è¯„ä¼°æŠ¥å‘Š")
        report.append(f"\nè¯„ä¼°æ—¶é—´: {results.get('metadata', {}).get('evaluation_date', 'Unknown')}")
        report.append(f"ä½¿ç”¨æ¨¡å‹: {results.get('metadata', {}).get('model_used', 'Unknown')}")
        report.append(f"è¯„åˆ†èŒƒå›´: 1-{results.get('metadata', {}).get('scoring_scale', 5)}")
        
        report.append(f"\n## æ€»ä½“è¯„åˆ†: {results.get('overall_score', 0):.2f}/5.00")
        
        report.append("\n## å„ç»´åº¦è¯¦ç»†è¯„åˆ†")
        for aspect, result in results.items():
            if aspect not in ['overall_score', 'metadata']:
                score = result.get('score', 0)
                reasoning = result.get('reasoning', '')
                report.append(f"\n### {aspect.title()}: {score:.2f}/5.00")
                report.append(f"**ç†ç”±**: {reasoning}")
        
        report.append("\n## æ”¹è¿›å»ºè®®")
        suggestions = self.generate_improvement_suggestions(results)
        for suggestion in suggestions:
            report.append(f"- {suggestion}")
        
        return "\n".join(report)
    
    def generate_improvement_suggestions(self, results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        for aspect, result in results.items():
            if aspect not in ['overall_score', 'metadata']:
                score = result.get('score', 3.0)
                if score < 3.0:
                    if aspect == 'coherence':
                        suggestions.append("æé«˜è¯é¢˜å†…éƒ¨è¿è´¯æ€§ï¼šç¡®ä¿å…³é”®è¯è¯­ä¹‰ç›¸å…³ï¼Œè¯é¢˜æè¿°å‡†ç¡®")
                    elif aspect == 'consistency':
                        suggestions.append("æ”¹å–„è¯é¢˜ä¸€è‡´æ€§ï¼šæ˜ç¡®è¯é¢˜è¾¹ç•Œï¼Œé¿å…é‡å ")
                    elif aspect == 'fluency':
                        suggestions.append("æå‡è¡¨è¾¾æµç•…æ€§ï¼šä¼˜åŒ–è¯é¢˜æè¿°å’Œå…³é”®è¯è¡¨è¾¾")
                    elif aspect == 'relevance':
                        suggestions.append("å¢å¼ºç›¸å…³æ€§ï¼šç¡®ä¿è¯é¢˜å‡†ç¡®åæ˜ æ–‡æ¡£å†…å®¹")
                    elif aspect == 'diversity':
                        suggestions.append("å¢åŠ è¯é¢˜å¤šæ ·æ€§ï¼šæ¶µç›–æ›´å¤šä¸åŒä¸»é¢˜é¢†åŸŸ")
        
        if not suggestions:
            suggestions.append("å½“å‰è¯é¢˜å»ºæ¨¡ç»“æœè´¨é‡è‰¯å¥½ï¼Œå¯ä»¥ä¿æŒç°æœ‰è®¾ç½®")
        
        return suggestions

def main():
    """æµ‹è¯•G-Evalè¿è¡Œå™¨"""
    # åˆ›å»ºè¿è¡Œå™¨
    geval_runner = CustomGEvalRunner()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆå‡è®¾å·²æœ‰TopicGPTç»“æœï¼‰
    if os.path.exists('data/output/topicgpt_results/topicgpt_results.json'):
        geval_input = geval_runner.prepare_geval_input(
            'data/output/topicgpt_results/topicgpt_results.json',
            'data/input/dataset.jsonl'
        )
        
        # è¿è¡Œè¯„ä¼°
        results = geval_runner.run_evaluation(geval_input)
        
        # ä¿å­˜ç»“æœ
        geval_runner.save_evaluation_results(results, 'results/geval_results.json')
        
        # ç”ŸæˆæŠ¥å‘Š
        report = geval_runner.generate_evaluation_report(results)
        with open('results/evaluation_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("G-Evalè¯„ä¼°å®Œæˆï¼")
    else:
        print("æœªæ‰¾åˆ°TopicGPTç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œè¯é¢˜å»ºæ¨¡")

if __name__ == "__main__":
    main() 