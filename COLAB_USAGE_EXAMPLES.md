# ğŸš€ Colabä½¿ç”¨ç¤ºä¾‹ - çµæ´»çš„å‚æ•°æ§åˆ¶

## ğŸ“‹ åŸºæœ¬ç”¨æ³•

### 1. å…‹éš†ä»£ç å¹¶å®‰è£…ä¾èµ–

```python
# å…‹éš†ä½ çš„GitHubä»“åº“
!git clone https://github.com/yiyangliu3218/LLM_TP.git
%cd LLM_TP

# å®‰è£…ä¾èµ–
!pip install -r requirements.txt
!pip install transformers torch accelerate bitsandbytes
!pip install sentence-transformers scikit-learn
!pip install matplotlib seaborn plotly umap-learn hdbscan wordcloud
```

### 2. ä¸Šä¼ æ•°æ®æ–‡ä»¶

```python
from google.colab import files
uploaded = files.upload()  # ä¸Šä¼ mydata.jsonlæ–‡ä»¶
```

## ğŸ¯ ä½¿ç”¨main.pyï¼ˆå®Œæ•´åŠŸèƒ½ï¼‰

### ç¤ºä¾‹1ï¼šè¿è¡ŒLlama 3è¯é¢˜å»ºæ¨¡

```python
# ä½¿ç”¨Llama 3å¼€æºæ¨¡å‹
!python main.py --mode topic_modeling --api_type llama3 --num_topics 10 --sample_size 300
```

### ç¤ºä¾‹2ï¼šè¿è¡Œå®Œæ•´é—­ç¯ç³»ç»Ÿ

```python
# è¿è¡Œå®Œæ•´çš„é—­ç¯ä¼˜åŒ–ç³»ç»Ÿ
!python main.py --mode closed_loop --api_type llama3 --num_topics 8 --max_iterations 3
```

### ç¤ºä¾‹3ï¼šä»…è¿è¡Œæ•°æ®é¢„å¤„ç†

```python
# åªè¿›è¡Œæ•°æ®é¢„å¤„ç†
!python main.py --mode preprocess --data_path mydata.jsonl --sample_size 500
```

### ç¤ºä¾‹4ï¼šä»…è¿è¡Œè¯„ä¼°

```python
# åªè¿›è¡Œè´¨é‡è¯„ä¼°
!python main.py --mode evaluation
```

## ğŸ¦™ ä½¿ç”¨colab_main.pyï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰

### ç¤ºä¾‹1ï¼šLlama 3è¯é¢˜å»ºæ¨¡

```python
# ä½¿ç”¨Llama 3è¿›è¡Œè¯é¢˜å»ºæ¨¡
!python colab_main.py --mode llama3 --num_topics 8 --sample_size 500
```

### ç¤ºä¾‹2ï¼šSentence Transformersè¯é¢˜å»ºæ¨¡

```python
# ä½¿ç”¨sentence-transformersè¿›è¡Œè¯é¢˜å»ºæ¨¡
!python colab_main.py --mode sentence_transformers --num_topics 6 --sample_size 300
```

### ç¤ºä¾‹3ï¼šæ¯”è¾ƒä¸¤ç§æ–¹æ³•

```python
# åŒæ—¶è¿è¡Œä¸¤ç§æ–¹æ³•å¹¶æ¯”è¾ƒç»“æœ
!python colab_main.py --mode both --num_topics 8 --sample_size 400
```

## ğŸ”§ å‚æ•°è¯´æ˜

### main.py å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | é€‰é¡¹ |
|------|------|--------|------|
| `--mode` | è¿è¡Œæ¨¡å¼ | `closed_loop` | `preprocess`, `topic_modeling`, `evaluation`, `closed_loop` |
| `--api_type` | APIç±»å‹ | `llama3` | `openai`, `huggingface`, `local`, `llama3` |
| `--model_name` | æ¨¡å‹åç§° | `meta-llama/Meta-Llama-3-8B-Instruct` | ä»»æ„æ¨¡å‹åç§° |
| `--num_topics` | è¯é¢˜æ•°é‡ | `8` | 1-20 |
| `--sample_size` | é‡‡æ ·å¤§å° | `500` | 100-1000 |
| `--max_iterations` | æœ€å¤§è¿­ä»£æ¬¡æ•° | `3` | 1-10 |
| `--temperature` | æ¸©åº¦å‚æ•° | `0.7` | 0.1-1.0 |

### colab_main.py å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | é€‰é¡¹ |
|------|------|--------|------|
| `--mode` | è¿è¡Œæ¨¡å¼ | `llama3` | `llama3`, `sentence_transformers`, `both` |
| `--num_topics` | è¯é¢˜æ•°é‡ | `8` | 1-20 |
| `--sample_size` | é‡‡æ ·å¤§å° | `500` | 100-1000 |

## ğŸ¯ å®é™…ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šå¿«é€Ÿæ¢ç´¢ï¼ˆå°æ•°æ®é›†ï¼‰

```python
# å¿«é€Ÿæµ‹è¯•ï¼Œä½¿ç”¨å°æ•°æ®é›†
!python colab_main.py --mode both --num_topics 5 --sample_size 200
```

### åœºæ™¯2ï¼šè¯¦ç»†åˆ†æï¼ˆå¤§æ•°æ®é›†ï¼‰

```python
# è¯¦ç»†åˆ†æï¼Œä½¿ç”¨å¤§æ•°æ®é›†
!python main.py --mode closed_loop --api_type llama3 --num_topics 12 --sample_size 800 --max_iterations 5
```

### åœºæ™¯3ï¼šå¯¹æ¯”å®éªŒ

```python
# å¯¹æ¯”ä¸åŒå‚æ•°çš„æ•ˆæœ
!python colab_main.py --mode llama3 --num_topics 6 --sample_size 300
!python colab_main.py --mode llama3 --num_topics 8 --sample_size 300
!python colab_main.py --mode llama3 --num_topics 10 --sample_size 300
```

### åœºæ™¯4ï¼šèµ„æºä¼˜åŒ–

```python
# å†…å­˜ä¸è¶³æ—¶ä½¿ç”¨è¾ƒå°çš„å‚æ•°
!python colab_main.py --mode sentence_transformers --num_topics 6 --sample_size 200
```

## ğŸ“Š ç»“æœæŸ¥çœ‹

### æŸ¥çœ‹ç”Ÿæˆçš„è¯é¢˜

```python
import json

# è¯»å–Llama 3ç»“æœ
with open('llama3_topic_modeling_results.json', 'r', encoding='utf-8') as f:
    llama_results = json.load(f)

print("Llama 3è¯é¢˜:")
for topic_id, topic in llama_results['topics'].items():
    print(f"{topic_id}: {topic['title']}")
    print(f"  å…³é”®è¯: {', '.join(topic['keywords'][:5])}")
    print(f"  æ–‡æ¡£æ•°: {topic['size']}")
```

### æŸ¥çœ‹è¯„ä¼°ç»“æœ

```python
# æŸ¥çœ‹è´¨é‡è¯„ä¼°
print(f"æ€»ä½“è¯„åˆ†: {llama_results['overall_score']:.2f}/5.0")
for aspect, score in llama_results['evaluation'].items():
    print(f"{aspect}: {score:.1f}/5.0")
```

### ä¸‹è½½ç»“æœæ–‡ä»¶

```python
from google.colab import files

# ä¸‹è½½ç»“æœæ–‡ä»¶
files.download('llama3_topic_modeling_results.json')
files.download('sentence_transformers_results.json')
```

## ğŸ†˜ å¸¸è§é—®é¢˜è§£å†³

### å†…å­˜ä¸è¶³

```python
# å‡å°‘é‡‡æ ·å¤§å°
!python colab_main.py --mode llama3 --sample_size 200

# æˆ–ä½¿ç”¨æ›´è½»é‡çš„æ–¹æ³•
!python colab_main.py --mode sentence_transformers --sample_size 200
```

### è¿è¡Œæ—¶é—´å¤ªé•¿

```python
# å‡å°‘è¯é¢˜æ•°é‡
!python colab_main.py --mode llama3 --num_topics 5

# æˆ–å‡å°‘é‡‡æ ·å¤§å°
!python colab_main.py --mode llama3 --sample_size 300
```

### æ¨¡å‹ä¸‹è½½å¤±è´¥

```python
# ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
!python main.py --mode topic_modeling --api_type sentence_transformers
```

## ğŸ‰ ä¸€é”®è¿è¡Œç¤ºä¾‹

```python
# å®Œæ•´çš„ä¸€é”®è¿è¡Œç¤ºä¾‹
import os
from google.colab import files

print("ğŸš€ å¼€å§‹è¯é¢˜å»ºæ¨¡...")

# 1. å…‹éš†ä»“åº“
!git clone https://github.com/yiyangliu3218/LLM_TP.git
%cd LLM_TP

# 2. å®‰è£…ä¾èµ–
!pip install -r requirements.txt
!pip install transformers torch accelerate bitsandbytes
!pip install sentence-transformers scikit-learn
!pip install matplotlib seaborn plotly umap-learn hdbscan wordcloud

# 3. ä¸Šä¼ æ•°æ®
uploaded = files.upload()

if 'mydata.jsonl' in uploaded:
    print("âœ… æ•°æ®ä¸Šä¼ æˆåŠŸï¼")
    
    # 4. è¿è¡Œè¯é¢˜å»ºæ¨¡ï¼ˆé€‰æ‹©ä¸€ç§æ–¹æ³•ï¼‰
    # æ–¹æ³•A: Llama 3
    !python colab_main.py --mode llama3 --num_topics 8 --sample_size 500
    
    # æ–¹æ³•B: Sentence Transformers
    # !python colab_main.py --mode sentence_transformers --num_topics 8 --sample_size 500
    
    # æ–¹æ³•C: æ¯”è¾ƒä¸¤ç§æ–¹æ³•
    # !python colab_main.py --mode both --num_topics 8 --sample_size 500
    
    # 5. ä¸‹è½½ç»“æœ
    files.download('llama3_topic_modeling_results.json')
    print("ğŸ‰ å®Œæˆï¼")
else:
    print("âŒ è¯·ä¸Šä¼ mydata.jsonlæ–‡ä»¶")
```

---

**ğŸ¯ ç°åœ¨ä½ å¯ä»¥çµæ´»åœ°ä½¿ç”¨ä¸åŒçš„å‚æ•°æ¥æ§åˆ¶è¯é¢˜å»ºæ¨¡çš„åŠŸèƒ½äº†ï¼** 